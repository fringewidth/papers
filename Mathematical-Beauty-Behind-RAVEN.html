<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Explore how graph theory revolutionizes dimensionality reduction by modeling dataset features as a graph to streamline big data efficiently.">
    <meta name="keywords"
        content="graph theory, dimensionality reduction, data science, machine learning, feature selection, PCA alternative">
    <meta name="author" content="Hrishik Sai Bojnal">
    <meta name="google-site-verification" content="ROR3XXyRauPsVJs2Usq4wNUO0MBo3HpscXT4inpTAww" />
    <title>Using Graph Theory for Dimensionality Reduction</title>
    <link rel="stylesheet" href="styles.css">

    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']]
            }
        };
    </script>
</head>

<body>
    <header>
        <h1>Why Graph Theory Makes Feature Selection Work</h1>
        <p class="subtitle">The Mathematical Beauty Behind RAVEN</p>

        <div class="buttons">
            <a href="https://drive.google.com/file/d/1D6dzpmQe6o1U1X3-4uvdHB287tvvCE2a/view" target="_blank"
                class="standing button">
                Full Preprint</a>
            <a href="https://github.com/fringewidth/raven" target="_blank" class="standing button">Code</a>

        </div>
    </header>

    <main>


        <p>RAVEN (Redundancy Analysis via Elimination Networks) is a feature selection algorithm that achieves something
            remarkable: it reduces high-dimensional datasets by 41-69% while maintaining or even <em>improving</em>
            model performance. On ClimSim, a climate modeling dataset with 556 features, RAVEN eliminated 228 features
            while preserving R² scores for temperature predictions. On the UCI SECOM dataset, it improved classification
            ROC AUC from 0.54 (with PCA) to 0.97. For RNA sequencing data with 35,238 genes, it achieved a 69% reduction
            while maintaining a silhouette score of 0.9845.</p>
        <p>These results seem almost too good to be true. How can we throw away so much data and lose so little, or
            sometimes even gain predictive power?</p>
        <p>While it started as a solution to make ClimSim small enough to fit on my laptop&#39;s RAM, the answer lies in
            a beautiful mathematical proof that brings the perfect world of graph theory to the imperfections of
            real-world machine learning. This post walks you through that.</p>
        <h2 id="r-scores-and-information-loss">R² Scores and Information Loss</h2>
        <p>The R² score between two features measures their shared information. When two features have R² = 0.99, one
            explains 99% of the other&#39;s variance—they&#39;re nearly redundant. So what happens when there are three
            features? If A explains 99% of B&#39;s variance, and B explains 50% of C&#39;s variance, what can be said
            about the relationship between A and C?</p>
        <p>It turns out that correlation is notoriously <strong>non-transitive</strong>. We can&#39;t simply conclude
            that A correlates strongly with C. This makes multi-feature redundancy analysis treacherous.</p>
        <p>RAVEN is a <strong>threshold-based</strong> algorithm that transforms this seemingly intractable problem into
            one with provable guarantees. By requiring all pairwise correlations in a feature chain (i.e., A→B→C) to
            exceed a high threshold τ (typically &gt; 0.95), we can bound information decay across the chain.</p>
        <h2 id="feature-chains-and-correlation-decay">Feature Chains and Correlation Decay</h2>
        <p>Let&#39;s formalize this. A <strong>feature chain</strong> is a sequence X₁, X₂, ..., Xₙ where adjacent
            features have squared correlation ρ²ᵢ,ᵢ₊₁ ≥ τ for some threshold τ &gt; 1 - ε (where ε is small, like 0.01),
            and all correlations have the same sign.</p>
        <p>The main question is: <em>If we only keep the first feature in a chain, how much information about the last
                feature do we lose?</em></p>
        <p><strong>Theorem (Transitive Correlation)</strong>: For any two features Xᵢ and Xᵢ₊ₖ in a feature chain, their
            squared correlation satisfies $\rho_{i,i+k}^2 = \Omega(\tau^k)$.</p>
        <p><em>(If you need a quick refresher from algorithms class, 
the <a href="https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-omega-notation">
Omega notation</a> describes a lower limit on how fast a function can grow. 
In other words, $f(x) = \Omega(g(x))$ means that beyond a certain point, 
$f(x)$ stays at least as large as some constant times $g(x)$.)</em></p>

        <p>The theorem essentially says that correlation decays <strong>exponentially</strong> with chain length, but in
            a <em>bounded</em> way controlled by τ. Let&#39;s see why this is true.</p>
        <h2 id="the-proof-how-the-dominoes-fall-">The Proof: How the dominoes fall!</h2>
        <p>We use everyone&#39;s proof method: induction.</p>
        <h3 id="base-case">Base Case</h3>
        <p>For k = 1, the result is immediate from the definition: $\rho_{i,i+1}^2 \ge \tau$
            by construction.</p>
        <h3 id="inductive-step">Inductive Step</h3>
        <p>Assume the theorem holds for some distance m ≥ 2: $\rho_{i,i+m}^2 = \Omega(\tau^m)$
            . We need to show it holds for m+1. That is, $\rho_{i,i+m+1}^2 = \Omega(\tau^{m+1})$ </p>
        <p>We start with a surprisingly less known result from <a href="https://doi.org/10.1198/000313001753272286">Langford
                (2001)</a> on correlation bounds. For three random variables X, Y, Z:</p>
        <p>
            $$\rho_{XZ} \ge \rho_{XY} \cdot \rho_{YZ} - \sqrt{(1 - \rho_{XY}^2)(1 - \rho_{YZ}^2)}$$
        </p>

        <p>This inequality gives us a lower bound on the correlation between X and Z through an intermediary Y. Applying
            this to our chain at position i, i+m, i+m+1:</p>
        <p>$$\rho_{i,i+m+1} \ge \rho_{i,i+m} \cdot \rho_{i+m,i+m+1} - \sqrt{(1 - \rho_{i,i+m}^2)(1 -
            \rho_{i+m,i+m+1}^2)}$$</p>
        <p>Now the rather clever part: by the inductive hypothesis, ∃c₁ such that $\rho_{i,i+m}^2 \ge c_1 \tau^m$,
            which means $\lvert \rho_{i,i+m} \rvert \ge \sqrt{c_1 \tau^m}$. By the feature chain definition,
            $\lvert \rho_{i+m,i+m+1} \rvert \ge \sqrt{\tau}$. Since both correlations have the same sign, we can
            multiply them:</p>
        <p>$$\rho_{i,i+m} \cdot \rho_{i+m,i+m+1} \ge \sqrt{c_1 \tau^m} \cdot \sqrt{\tau} = c_1&#39;
            \tau^{(m+1)/2}$$</p>
        <p>For the second term (the &quot;penalty&quot; term), we have:</p>
        <p>$$\sqrt{(1 - \rho_{i,i+m}^2)(1 - \rho_{i+m,i+m+1}^2)} \le \sqrt{(1 - c_1 \tau^m)(1 - \tau)}$$</p>
        <h2 id="the-beautiful-asymptotic-argument">The Beautiful Asymptotic Argument</h2>
        <p>Here&#39;s where the mathematics becomes elegant. Since $\tau &gt; 1 - \varepsilon$ for small $\varepsilon$,
            we have $1 - \tau &lt; \varepsilon$. For large $m$,
            $\tau^m &gt; (1 - \varepsilon)^m \approx 1 - m\varepsilon$ (first-order Taylor expansion). So the penalty
            term is bounded by:</p>
        <p>$$\sqrt{\varepsilon - c_1 \varepsilon (1 - m \varepsilon)} = \sqrt{\varepsilon (1 - c_1 (1 - m
            \varepsilon))}$$
        </p>
        <p>This is subexponential in $m$. The dominant term is $c_1&#39; \tau^{(m+1)/2}$, giving:</p>
        <p>$$\rho_{i,i+m+1} = \Omega(\tau^{(m+1)/2})$$</p>
        <p>Squaring both sides: $\rho_{i,i+m+1}^2 = \Omega(\tau^{m+1})$ !!!</p>
        <h2 id="from-chains-to-graphs">From Chains to Graphs</h2>
        <p>Real datasets are complex networks of interdependencies. When we construct a <strong>feature network</strong>
            G(τ) where vertices are features and edges connect features with R² ≥ τ, something magical happens at high
            thresholds: the graph fragments into <strong>connected components</strong>.</p>
        <p>Each connected component is what RAVEN calls an <strong>elimination network</strong>: a cluster of mutually
            correlated features. The theorem above extends naturally: the information preserved between any two features
            in a network is bounded by τ^(shortest_path_distance).</p>
        <p>This leads to a clean optimization problem: which single feature should we keep from each elimination
            network?</p>
        <h2 id="closeness-centrality">Closeness Centrality</h2>
        <p>RAVEN defines the <strong>Minimum Shared Explained Variance</strong> (MSEV) for a feature X as:</p>
        <p>$$\mathrm{MSEV}(X) = \frac{1}{n-1} \sum_Y \tau^{d(X,Y)}$$</p>
        <p>where d(X,Y) is the shortest path distance between features. This represents the average information X
            preserves about all other features in the network.</p>
        <p>What&#39;s remarkable is that <strong>maximizing MSEV is equivalent to finding the <a
                    href="https://en.wikipedia.org/wiki/Closeness_centrality">closeness center</a> of the
                graph</strong>, the vertex that minimizes average distance to all others.</p>
        <p>Why? Because maximizing $\sum_Y \tau^{d(X,Y)}$
            is equivalent to minimizing $\sum_Y d(X,Y)$ (since τ &lt; 1, larger distances contribute less). The
            closeness centrality is defined as:</p>
        <p>$$C(X) = \frac{n-1}{\sum_Y d(X,Y)}$$</p>
        <p>The feature maximizing C(X) is precisely the one maximizing MSEV. Graph theory gives us the optimal feature
            selector!</p>
        <h2 id="where-did-i-even-come-up-with-this-">Where did I even come up with this?</h2>
        <p>I created this algorithm purely from an intuitive sense to make ClimSim small enough to experiment with
            transformer-based U-Nets for weather prediction. I figured temperature measurements at altitude levels 1000m
            and 1010m are probably redundant—keep one representative level.</p>
        <p>What I didn&#39;t expect was how <em>well</em> it would work. A 41% reduction with negligible performance
            loss seemed..too good to be true. So I dug into the mathematics. I found Langford&#39;s inequality in the
            depths of the internet, where I&#39;m genuinely surprised it&#39;s not part of a larger curriculum of
            statistics (maybe it is, just not in whatever sources I use to learn).</p>
        <p>So why does it work with Climsim? In datasets with natural physical hierarchies (like atmospheric
            measurements at different altitudes), features form chains and clusters with very high pairwise
            correlations. When τ = 0.99, we&#39;re guaranteeing that keeping the closeness center preserves at least
            0.99^d of the information about any feature at distance d.</p>
        <p>For a chain of 10 temperature measurements, the furthest feature is at distance 9, so we preserve at least
            0.99^9 ≈ 0.91 of its variance. That&#39;s a theoretical guarantee of less than 9% information loss. In
            practice, the loss is often much smaller because features have multiple paths in the network.</p>
        <p>The proof shows why graph-based feature selection works: correlation networks naturally capture semantic
            relationships (like &quot;measurements of the same phenomenon&quot;), and closeness centrality provably
            identifies the most representative feature. The exponential decay bound ( $τ^k$ ) is tight enough to
            guarantee
            low information loss, yet loose enough to work with many datasets.</p>
        <p>This is the beauty of applied mathematics in machine learning: an intuitive algorithm that <em>works</em> can
            reveal deep mathematical structure. And that mathematical structure, in turn, explains why the intuition was
            correct.</p>
        <p>The next time you see correlation matrices in your data, look for the chains and clusters. We made RAVEN an
            importable python function, which you can find <a href="https://github.com/fringewidth/raven">here</a></p>
    </main>
    <footer>
        <p>&copy; 2025 Hrishik Sai Bojnal. All rights reserved.</p>
    </footer>
</body>

</html>
